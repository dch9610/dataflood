{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 필요 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                                          #데이터 분석 라이브러리\n",
    "from sklearn.decomposition import PCA                        #주성분 분석 라이브러리\n",
    "import numpy as np                                           # 계산 라이브러리\n",
    "from sklearn.preprocessing import MinMaxScaler               # 최소 최대 0~1 범위로 변환을 위한 라이브러리\n",
    "\n",
    "from sklearn.model_selection import train_test_split         # 데이터 분할 라이브러리\n",
    "from sklearn.model_selection import cross_validate           # Kfold 라이브러리\n",
    "from sklearn.model_selection import cross_val_score          # Kfold 라이브러리\n",
    "from sklearn.model_selection import StratifiedKFold          # Kfold 라이브러리\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier           # KNN 라이브러리\n",
    "from sklearn.svm import SVC                                  # SVM Classifier 라이브러리\n",
    "from sklearn.tree import DecisionTreeClassifier              # 결정나무분류 라이브러리\n",
    "from sklearn.linear_model import LogisticRegression          # 로지스틱 회귀 라이브러리\n",
    "from bayes_opt import BayesianOptimization                   # 베이시안 라이브러리   \n",
    "\n",
    "\n",
    "from sklearn.metrics import make_scorer                      # 평가용 라이브러리\n",
    "from sklearn.metrics import f1_score                         # f1  라이브러리\n",
    "from sklearn.metrics import accuracy_score                   # accuracy 라이브러리 \n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt                              # 그래프 라이브러리\n",
    "from sklearn.ensemble import VotingClassifier                # 보팅 앙상블 라이브러리\n",
    "import seaborn as sns                                        # boxplot 을 위한 라이브러리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "File = pd.read_csv(\"Weighted_data_nonscale.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Floodfile = File.drop(['Unnamed: 0','FLOOD'], axis = 1)\n",
    "Floodclass = File['FLOOD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Floodfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# minmaxscaler\n",
    "## data scale 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(Floodfile)\n",
    "Floodfile = scaler.transform(Floodfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=4) # 주성분을 몇개로 할지 결정\n",
    "printcipalComponents = pca.fit_transform(Floodfile)\n",
    "Floodfile = pd.DataFrame(data=printcipalComponents, columns = ['principal component1', 'principal component2','principal component3', 'principal component4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Floodfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data split\n",
    "## 8 : 2 으로 training 과 test 구분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(Floodfile, Floodclass , test_size=0.2, random_state=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skfolds=StratifiedKFold(n_splits=3, random_state=400 ,shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BayesianOptimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "KNN_search_point = {'k': (3,50), 'leaf': (1,50)}\n",
    "\n",
    "gp_params = {\"alpha\":1e-10}\n",
    "scores_all = []\n",
    "k_all = []\n",
    "leaf_all=[]\n",
    "\n",
    "def KNN_opt(k,leaf):\n",
    "    cvscores = []\n",
    "    \n",
    "    for train_index, val_index in skfolds.split(x_train, y_train):\n",
    "#         print(train_index)\n",
    "#         print(val_index)\n",
    "        x_skftrain, x_val = x_train.values[train_index], x_train.values[val_index]\n",
    "        Y_skftrain, Y_val = np.array(y_train)[train_index],np.array(y_train)[val_index]\n",
    "        \n",
    "        KNNmodel = KNeighborsClassifier(n_neighbors = int(k),weights = 'distance', leaf_size= leaf ) # KNN  \n",
    "        \n",
    "        KNNmodel2 = KNNmodel.fit(x_skftrain,Y_skftrain)      # 모델학습\n",
    "        y_pred = KNNmodel2.predict(x_val)                    # 예측\n",
    "\n",
    "        report = f1_score(Y_val, y_pred, average='weighted') # F1\n",
    "#         report = accuracy_score(Y_val, y_pred)             # accuracy\n",
    "        cvscores.append(report)\n",
    "    \n",
    "    \n",
    "    scores_all.append(np.mean(cvscores))\n",
    "    k_all.append(int(k))\n",
    "#     leaf_all.append(int(leaf))\n",
    "    \n",
    "    return np.mean(cvscores)\n",
    "\n",
    "result = BayesianOptimization(KNN_opt, KNN_search_point,    random_state=10,    verbose=2) \n",
    "result.maximize(acq='ucb',n_iter=15, kappa=2.576, init_points=1 , **gp_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(k_all, scores_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_search_point = {'k': (1,50), 'gamma': (0.001,20)}\n",
    "\n",
    "gp_params = {\"alpha\":1e-10}\n",
    "scores_all = []\n",
    "k_all = []\n",
    "gamma_all = []\n",
    "def SVM_opt(k,gamma):\n",
    "    cvscores = []\n",
    "    \n",
    "    for train_index, val_index in skfolds.split(x_train, y_train):\n",
    "        x_skftrain, x_val = x_train.values[train_index],x_train.values[val_index]\n",
    "        Y_skftrain, Y_val = np.array(y_train)[train_index],np.array(y_train)[val_index]\n",
    "\n",
    "        SVMmodel = SVC( C= int(k) , kernel='rbf' , probability = True, gamma = gamma ) # SVM  \n",
    "        \n",
    "        SVMmodel2 = SVMmodel.fit(x_skftrain,Y_skftrain)      # 모델학습\n",
    "        y_pred = SVMmodel2.predict(x_val)                    # 예측\n",
    "\n",
    "        report = f1_score(Y_val, y_pred, average='weighted') # F1\n",
    "#         report = accuracy_score(Y_val, y_pred)             # accuracy\n",
    "        cvscores.append(report)\n",
    "    \n",
    "    scores_all.append(np.mean(cvscores))\n",
    "    k_all.append(int(k))\n",
    "    return np.mean(cvscores)\n",
    "\n",
    "result = BayesianOptimization(SVM_opt, SVM_search_point, random_state=10, verbose=2) \n",
    "result.maximize(acq='ucb',n_iter=15, kappa=2.576, init_points=1 , **gp_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_search_point = {'k': (1,50)}\n",
    "\n",
    "gp_params = {\"alpha\":1e-10}\n",
    "scores_all = []\n",
    "k_all = []\n",
    "gamma_all = []\n",
    "def SVM_opt(k):\n",
    "    cvscores = []\n",
    "    \n",
    "    for train_index, val_index in skfolds.split(x_train, y_train):\n",
    "        x_skftrain, x_val = x_train.values[train_index],x_train.values[val_index]\n",
    "        Y_skftrain, Y_val = np.array(y_train)[train_index],np.array(y_train)[val_index]\n",
    "\n",
    "        SVMmodel = SVC( C= int(k) , kernel='poly' , probability = True ) # SVM  \n",
    "        \n",
    "        SVMmodel2 = SVMmodel.fit(x_skftrain,Y_skftrain)      # 모델학습\n",
    "        y_pred = SVMmodel2.predict(x_val)                    # 예측\n",
    "\n",
    "        report = f1_score(Y_val, y_pred, average='weighted') # F1\n",
    "#         report = accuracy_score(Y_val, y_pred)             # accuracy\n",
    "        cvscores.append(report)\n",
    "    \n",
    "    scores_all.append(np.mean(cvscores))\n",
    "    k_all.append(int(k))\n",
    "    return np.mean(cvscores)\n",
    "\n",
    "result = BayesianOptimization(SVM_opt, SVM_search_point, random_state=10, verbose=2) \n",
    "result.maximize(acq='ucb',n_iter=15, kappa=2.576, init_points=1 , **gp_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(k_all, scores_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_search_point = { 'k': (1,30), 'leaf' : ( 2, 30), 'split': (2,30) }\n",
    "\n",
    "gp_params = {\"alpha\":1e-10}\n",
    "scores_all = []\n",
    "k_all = []\n",
    "leaf_all = []\n",
    "split_all = []\n",
    "\n",
    "def DT_opt(k,leaf,split):\n",
    "    cvscores = []\n",
    "    \n",
    "    for train_index, val_index in skfolds.split(x_train, y_train):\n",
    "        x_skftrain, x_val = x_train.values[train_index],x_train.values[val_index]\n",
    "        Y_skftrain, Y_val = np.array(y_train)[train_index],np.array(y_train)[val_index]\n",
    "\n",
    "        DTmodel = DecisionTreeClassifier(criterion='gini',\n",
    "                                         max_depth=int(k),              # 트리의 최대 깊이\n",
    "                                         min_samples_leaf= int(leaf),\n",
    "                                         min_samples_split= int(split)) #DT  \n",
    "        \n",
    "        \n",
    "        DTmodel2 = DTmodel.fit(x_skftrain,Y_skftrain)      # 모델학습\n",
    "        y_pred = DTmodel2.predict(x_val)                    # 예측\n",
    "\n",
    "        report = f1_score(Y_val, y_pred, average='weighted') # F1\n",
    "#         report = accuracy_score(Y_val, y_pred)             # accuracy\n",
    "        cvscores.append(report)\n",
    "    \n",
    "    scores_all.append(np.mean(cvscores))\n",
    "    k_all.append(int(k))\n",
    "    leaf_all.append(int(leaf))\n",
    "    split_all.append(split)\n",
    "\n",
    "    \n",
    "    return np.mean(cvscores)\n",
    "\n",
    "result = BayesianOptimization(DT_opt, DT_search_point, random_state=10, verbose=2) \n",
    "result.maximize(acq='ucb',n_iter=15, kappa=2.576, init_points=1 , **gp_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(k_all, scores_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_search_point = { 'k': (1,50)}\n",
    "\n",
    "gp_params = {\"alpha\":1e-10}\n",
    "scores_all = []\n",
    "k_all = []\n",
    "\n",
    "def LR_opt(k):\n",
    "    cvscores = []\n",
    "    \n",
    "    for train_index, val_index in skfolds.split(x_train, y_train):\n",
    "        x_skftrain, x_val = x_train.values[train_index],x_train.values[val_index]\n",
    "        Y_skftrain, Y_val = np.array(y_train)[train_index],np.array(y_train)[val_index]\n",
    "\n",
    "        LRmodel = LogisticRegression(C= int(k) , multi_class= 'auto') #LR  \n",
    "        \n",
    "        \n",
    "        LRmodel2 = LRmodel.fit(x_skftrain,Y_skftrain)      # 모델학습\n",
    "        y_pred = LRmodel2.predict(x_val)                    # 예측\n",
    "\n",
    "        report = f1_score(Y_val, y_pred, average='weighted') # F1\n",
    "#         report = accuracy_score(Y_val, y_pred)             # accuracy\n",
    "        cvscores.append(report)\n",
    "    \n",
    "    scores_all.append(np.mean(cvscores))\n",
    "    k_all.append(int(k))\n",
    "    \n",
    "    return np.mean(cvscores)\n",
    "\n",
    "result = BayesianOptimization(LR_opt, LR_search_point, random_state=10, verbose=2) \n",
    "result.maximize(acq='ucb',n_iter=15, kappa=2.576, init_points=1 , **gp_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(k_all, scores_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting ensemble\n",
    "## 사용 된 모델 : KNN, SVM, Decision Tree, Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_model = KNeighborsClassifier(n_neighbors = 21,weights = 'distance', leaf_size=49)\n",
    "\n",
    "SVM_model = SVC( C= 29 , kernel='rbf' , probability = True , gamma = 30 )\n",
    "\n",
    "DT_model = DecisionTreeClassifier(criterion='gini', max_depth=28, min_samples_leaf= 5, min_samples_split= 2)\n",
    "\n",
    "LR_model = LogisticRegression(C= 5, multi_class= 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_model = KNeighborsClassifier()\n",
    "\n",
    "SVM_model = SVC(probability = True )\n",
    "\n",
    "DT_model = DecisionTreeClassifier()\n",
    "\n",
    "LR_model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble=VotingClassifier(\n",
    "        estimators=[('KNN',KNN_model),\n",
    "                   ('SVM',SVM_model),\n",
    "                   ('DT',DT_model),\n",
    "                   ('LR',LR_model)]\n",
    "        ,voting='soft')\n",
    "\n",
    "# Voting 모델 안에 각 모델들을 묶어주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_model.fit(x_train,y_train)\n",
    "SVM_model.fit(x_train,y_train)\n",
    "DT_model.fit(x_train,y_train)\n",
    "LR_model.fit(x_train,y_train)\n",
    "ensemble.fit(x_train,y_train)\n",
    "\n",
    "# 각 모델들을 데이터로 학습시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 교차검증 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKF = StratifiedKFold( n_splits = 3,  shuffle = True, random_state = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_scores=cross_validate(KNN_model, x_train, y_train, cv=SKF, scoring=make_scorer(f1_score))\n",
    "SVM_scores=cross_validate(SVM_model, x_train, y_train, cv=SKF, scoring=make_scorer(f1_score))\n",
    "DT_scores=cross_validate(DT_model, x_train, y_train, cv=SKF, scoring=make_scorer(f1_score))\n",
    "LR_scores=cross_validate(LR_model, x_train, y_train, cv=SKF, scoring=make_scorer(f1_score))\n",
    "ensemble_scores=cross_validate(ensemble, x_train, y_train, cv=SKF, scoring=make_scorer(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(KNN_scores)\n",
    "print(SVM_scores)\n",
    "print(DT_scores)\n",
    "print(LR_scores)\n",
    "print(ensemble_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_f1_score = KNN_scores[\"test_score\"].mean()\n",
    "SVN_f1_score = SVM_scores[\"test_score\"].mean()\n",
    "DT_f1_score = DT_scores[\"test_score\"].mean()\n",
    "LR_f1_score = LR_scores[\"test_score\"].mean()\n",
    "ensemble_f1_score = ensemble_scores[\"test_score\"].mean()\n",
    "\n",
    "\n",
    "print(\"f1_score: {0: .4f}\".format(KNN_f1_score))\n",
    "print(\"f1_score: {0: .4f}\".format(SVN_f1_score))\n",
    "print(\"f1_score: {0: .4f}\".format(DT_f1_score))\n",
    "print(\"f1_score: {0: .4f}\".format(LR_f1_score))\n",
    "print(\"f1_score: {0: .4f}\".format(ensemble_f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결과 boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "names = []\n",
    "\n",
    "models = []\n",
    "models.append(('KNN', KNN_model))\n",
    "models.append(('SVM', SVM_model))\n",
    "models.append(('DT', DT_model))\n",
    "models.append(('LR', LR_model))\n",
    "models.append(('Voting',ensemble))\n",
    "\n",
    "\n",
    "for name, model in models:\n",
    "    kfold = StratifiedKFold( n_splits = 3,  shuffle = True, random_state = 10)\n",
    "#     cv_results = cross_validate(ensemble, x_train, y_train, cv=SKF, scoring=make_scorer(f1_score))\n",
    "    cv_results = cross_val_score(ensemble, x_train, y_train, cv=SKF, scoring=make_scorer(f1_score))\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Classifier Comparison')\n",
    "ax= fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test 데이터 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_KNN = KNN_model.predict(x_test)\n",
    "pred_SVM = SVM_model.predict(x_test)\n",
    "pred_DT = DT_model.predict(x_test)\n",
    "pred_LR = LR_model.predict(x_test)\n",
    "\n",
    "# 각 모델들의 f1_score 파악을 위해 모델들 예측실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict= ensemble.predict(x_test)\n",
    "\n",
    "# 적용 모델인 Voting ensemble 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test data 교차검증 boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "names = []\n",
    "\n",
    "models = []\n",
    "models.append(('KNN', KNN_model))\n",
    "models.append(('SVM', SVM_model))\n",
    "models.append(('DT', DT_model))\n",
    "models.append(('LR', LR_model))\n",
    "models.append(('Voting',ensemble))\n",
    "\n",
    "\n",
    "for name, model in models:\n",
    "    kfold = StratifiedKFold( n_splits = 3,  shuffle = True, random_state = 10)\n",
    "#     cv_results = cross_validate(ensemble, x_train, y_train, cv=SKF, scoring=make_scorer(f1_score))\n",
    "    cv_results = cross_val_score(ensemble, x_test, y_test, cv=SKF, scoring=make_scorer(f1_score))\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Classifier Comparison')\n",
    "ax= fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020 년 사고사례 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accident = pd.read_excel(\"초량동 지형 강우량.xlsx\")   # 초량지하차도 지형정보 와 당일 시간당 강우량 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Busanaccident = accident.drop(['SLOPE_L','SLOPE_H','FLOOD','MANHOLES_RATIO'], axis = 1)  # 불필요 컬럼제거\n",
    "Busanaccidentclass = accident['FLOOD']                                                  # 분류구분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 가중치 주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Busanaccident.rename(columns={'R_W_SUM':'Hourly_Rainfall_Weight','HOUR_RAINFALL':'Hourly_Rainfall','IMP_SUR_RATIO':'Impervious_Surface',\n",
    "                              'IMP_W_SUM':'Impervious_Surface_Weight'},inplace=True)  # 컬럼명 수정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 강우량 가중치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,col in enumerate(Busanaccident['Hourly_Rainfall']):\n",
    "    if col <= 15:\n",
    "        Busanaccident['Hourly_Rainfall_Weight'][idx] = col / (46)\n",
    "    elif col > 15 and col <= 35:\n",
    "        Busanaccident['Hourly_Rainfall_Weight'][idx] = col / (18 * 2**2)\n",
    "    elif col > 35 and col <= 60:\n",
    "        Busanaccident['Hourly_Rainfall_Weight'][idx] = col / (17 * 3**2)\n",
    "    elif col > 60 and col <= 80:\n",
    "        Busanaccident['Hourly_Rainfall_Weight'][idx] = col / (11 * 4**2)\n",
    "    elif col > 80:\n",
    "        Busanaccident['Hourly_Rainfall_Weight'][idx] = col / (6.28 * 5**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 불투수면 가중치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,col in enumerate(Busanaccident['Impervious_Surface']):\n",
    "    if col <= 15:\n",
    "        Busanaccident['Impervious_Surface_Weight'][idx] = col / 6.66\n",
    "    elif col > 15 and col <= 25:\n",
    "        Busanaccident['Impervious_Surface_Weight'][idx] = col / (20 * 2)\n",
    "    elif col > 25 and col <= 40:\n",
    "        Busanaccident['Impervious_Surface_Weight'][idx] = col / (13.33 * 3)\n",
    "    elif col > 40 and col <= 55:\n",
    "        Busanaccident['Impervious_Surface_Weight'][idx] = col / (33.33 * 4)\n",
    "    elif col > 55:\n",
    "        Busanaccident['Impervious_Surface_Weight'][idx] = col / (26.67 * 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 경사도 가중치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Busanaccident['SLOPE_AVG_Weight'] = Busanaccident['SLOPE_AVG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,col in enumerate(Busanaccident['SLOPE_AVG']):\n",
    "    if col <= 10:\n",
    "        Busanaccident['SLOPE_AVG_Weight'][idx] = col / (41.67 / 5**2 )\n",
    "    elif col > 10 and col <= 15:\n",
    "        Busanaccident['SLOPE_AVG_Weight'][idx] = col / (25 / 4**2)\n",
    "    elif col > 15 and col <= 25:\n",
    "        Busanaccident['SLOPE_AVG_Weight'][idx] = col / (16.67 / 3**2)\n",
    "    elif col > 25 and col <= 50:\n",
    "        Busanaccident['SLOPE_AVG_Weight'][idx] = col / (8.33 / 2**2)\n",
    "    elif col > 50:\n",
    "        Busanaccident['SLOPE_AVG_Weight'][idx] = col / (8.33 / 1**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 고도 가중치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Busanaccident['HIGH_Weight'] = Busanaccident['HIGH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,col in enumerate(Busanaccident['HIGH']):\n",
    "    if col <= 20:\n",
    "        Busanaccident['HIGH_Weight'][idx] = col / (79.62 / 1**2)\n",
    "    elif col > 20 and col <= 40:\n",
    "        Busanaccident['HIGH_Weight'][idx] = col / (10.18 / 2**2)\n",
    "    elif col > 40 and col <= 60:\n",
    "        Busanaccident['HIGH_Weight'][idx] = col / (4.63 / 3**2)\n",
    "    elif col > 60 and col <= 80:\n",
    "        Busanaccident['HIGH_Weight'][idx] = col / (2.78 / 4**2)\n",
    "    elif col > 80:\n",
    "        Busanaccident['HIGH_Weight'][idx] = col / (2.78 / 5**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Busanaccident = Busanaccident.drop(['SLOPE_AVG','Impervious_Surface','Hourly_Rainfall','HIGH'], axis = 1)\n",
    "Busanaccident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(Busanaccident)\n",
    "Busanaccident = scaler.transform(Busanaccident)                         # sccler 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=4)\n",
    "printcipalComponents = pca.fit_transform(Busanaccident)\n",
    "Busanaccident = pd.DataFrame(data=printcipalComponents, columns = ['principal component1', 'principal component2','principal component3', 'principal component4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 예측값 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accident_predict= ensemble.predict(Busanaccident)                       # 위에서 학습된 모델 활용을 이용한 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accident_predict)                                                 # 예측값 확인"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
